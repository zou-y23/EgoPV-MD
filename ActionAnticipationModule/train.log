nohup: ignoring input
[INFO|2025-11-13 22:10:45] llamafactory.launcher:143 >> Initializing 5 distributed tasks at: 127.0.0.1:41829
/root/miniconda3/envs/LLama/lib/python3.10/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/root/miniconda3/envs/LLama/lib/python3.10/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/root/miniconda3/envs/LLama/lib/python3.10/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/root/miniconda3/envs/LLama/lib/python3.10/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/root/miniconda3/envs/LLama/lib/python3.10/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[W1113 22:10:51.466281137 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1113 22:10:51.466330336 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1113 22:10:51.466384240 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1113 22:10:51.466413152 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1113 22:10:52.604543550 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[INFO|2025-11-13 22:10:52] llamafactory.hparams.parser:468 >> Process rank: 1, world size: 5, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-11-13 22:10:52] llamafactory.hparams.parser:468 >> Process rank: 3, world size: 5, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-11-13 22:10:52] llamafactory.hparams.parser:468 >> Process rank: 2, world size: 5, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-11-13 22:10:52] llamafactory.hparams.parser:468 >> Process rank: 4, world size: 5, device: cuda:4, distributed training: True, compute dtype: torch.bfloat16
[WARNING|2025-11-13 22:10:52] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.
[INFO|2025-11-13 22:10:52] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-11-13 22:10:52] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 5, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2093] 2025-11-13 22:10:52,333 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2025-11-13 22:10:52,334 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2093] 2025-11-13 22:10:52,334 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2025-11-13 22:10:52,334 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2025-11-13 22:10:52,334 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2025-11-13 22:10:52,334 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2025-11-13 22:10:52,839 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-11-13 22:10:52,840 >> loading configuration file /root/autodl-tmp/Llama-33-70B-Instruct/config.json
[INFO|configuration_utils.py:839] 2025-11-13 22:10:52,844 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2093] 2025-11-13 22:10:52,845 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2025-11-13 22:10:52,845 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2093] 2025-11-13 22:10:52,845 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2025-11-13 22:10:52,845 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2025-11-13 22:10:52,845 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2025-11-13 22:10:52,845 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2025-11-13 22:10:53,326 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-11-13 22:10:53] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.
[INFO|2025-11-13 22:10:53] llamafactory.data.loader:143 >> Loading dataset prompt_for_gpt.json...
Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 70864 examples [00:00, 88786.21 examples/s]Generating train split: 70864 examples [00:00, 84329.46 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:05, 183.98 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 1966.99 examples/s]
/root/miniconda3/envs/LLama/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[rank0]:[W1113 22:10:55.292789215 ProcessGroupNCCL.cpp:5068] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:01<00:17, 53.28 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 126/1000 [00:01<00:08, 107.73 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 189/1000 [00:01<00:05, 159.81 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 252/1000 [00:01<00:03, 206.00 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▏      | 315/1000 [00:01<00:02, 246.49 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 378/1000 [00:02<00:02, 279.22 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 441/1000 [00:02<00:01, 305.66 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 504/1000 [00:02<00:01, 326.63 examples/s]Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 566/1000 [00:02<00:01, 340.59 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 628/1000 [00:02<00:01, 350.22 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 690/1000 [00:02<00:00, 357.04 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 752/1000 [00:03<00:00, 359.65 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 876/1000 [00:03<00:00, 410.39 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 938/1000 [00:03<00:00, 405.41 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:03<00:00, 399.80 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:03<00:00, 264.97 examples/s]
training example:
input_ids:
[128000, 128006, 9125, 128007, 271, 22818, 220, 23, 3347, 6299, 323, 264, 86180, 315, 279, 3347, 6299, 11, 499, 527, 2133, 311, 7168, 279, 1828, 1957, 304, 264, 3645, 315, 320, 23129, 11, 38021, 8, 6857, 13, 128009, 128006, 882, 128007, 271, 64379, 367, 25, 578, 5575, 4442, 279, 11863, 369, 279, 6122, 1360, 16016, 24561, 6299, 25, 17348, 342, 47026, 11, 6958, 11863, 80062, 11, 1825, 11863, 80062, 11, 17348, 342, 47026, 11, 11894, 11863, 11, 15142, 11863, 11, 17348, 11863, 11, 2035, 11863, 128009, 128006, 78191, 128007, 271, 36069, 11863, 128009]
inputs:
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Given 8 past actions and a narration of the past actions, you are going to predict the next action in a format of (verb, noun) pair.<|eot_id|><|start_header_id|>user<|end_header_id|>

Narration: The student changes the battery for the GoPro.; Past actions: rotate gopro, pull battery_door, open battery_door, rotate gopro, grab battery, withdraw battery, rotate battery, place battery<|eot_id|><|start_header_id|>assistant<|end_header_id|>

lift battery<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36069, 11863, 128009]
labels:
lift battery<|eot_id|>
[INFO|configuration_utils.py:763] 2025-11-13 22:11:00,198 >> loading configuration file /root/autodl-tmp/Llama-33-70B-Instruct/config.json
[INFO|configuration_utils.py:839] 2025-11-13 22:11:00,199 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|2025-11-13 22:11:00] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.
[INFO|2025-11-13 22:11:00] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[WARNING|logging.py:328] 2025-11-13 22:11:00,353 >> `torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:1169] 2025-11-13 22:11:02,198 >> loading weights file /root/autodl-tmp/Llama-33-70B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:2341] 2025-11-13 22:11:02,199 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-11-13 22:11:02,201 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:03<01:30,  3.11s/it]Loading checkpoint shards:   3%|▎         | 1/30 [00:03<01:28,  3.05s/it]Loading checkpoint shards:   3%|▎         | 1/30 [00:02<01:22,  2.84s/it]Loading checkpoint shards:   3%|▎         | 1/30 [00:02<01:23,  2.87s/it]Loading checkpoint shards:   3%|▎         | 1/30 [00:03<01:37,  3.36s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:07<01:46,  3.79s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:07<01:56,  4.16s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:07<01:50,  3.93s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:07<01:53,  4.06s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:08<02:02,  4.37s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:12<01:53,  4.22s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:12<01:59,  4.44s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:13<02:04,  4.62s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:13<02:08,  4.77s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:13<02:11,  4.87s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:16<01:55,  4.43s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:17<02:00,  4.65s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:18<02:05,  4.83s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:18<02:08,  4.94s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:19<02:11,  5.07s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:20<01:48,  4.35s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:22<01:55,  4.63s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:23<02:02,  4.89s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:23<02:03,  4.96s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:24<02:07,  5.08s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:25<01:42,  4.26s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:26<01:50,  4.59s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:28<01:56,  4.85s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:28<01:58,  4.93s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:29<01:36,  4.18s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:29<02:00,  5.04s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:31<01:44,  4.55s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:32<01:50,  4.80s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:33<01:51,  4.86s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:33<01:32,  4.21s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:34<01:54,  4.98s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:35<01:40,  4.58s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:37<01:47,  4.87s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:37<01:29,  4.27s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:38<01:48,  4.93s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:39<01:51,  5.05s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:40<01:37,  4.65s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:41<01:24,  4.22s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:42<01:44,  4.97s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:43<01:46,  5.05s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:44<01:48,  5.18s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:45<01:32,  4.64s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:46<01:19,  4.21s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:47<01:38,  4.93s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:48<01:39,  4.97s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:49<01:41,  5.09s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:49<01:27,  4.61s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:50<01:14,  4.17s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:52<01:33,  4.90s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:53<01:33,  4.94s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:54<01:22,  4.58s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:54<01:12,  4.26s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:54<01:35,  5.03s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:57<01:27,  4.85s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:58<01:27,  4.87s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:58<01:08,  4.30s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:59<01:18,  4.64s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:59<01:29,  4.96s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [01:02<01:23,  4.92s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [01:03<01:23,  4.92s/it]Loading checkpoint shards:  50%|█████     | 15/30 [01:02<01:02,  4.18s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [01:03<01:13,  4.61s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [01:04<01:24,  5.00s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [01:06<00:57,  4.10s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [01:07<01:18,  4.92s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [01:07<01:18,  4.91s/it]Loading checkpoint shards:  50%|█████     | 15/30 [01:07<01:08,  4.54s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [01:09<01:19,  4.99s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [01:10<00:52,  4.05s/it]Loading checkpoint shards:  50%|█████     | 15/30 [01:12<01:12,  4.85s/it]Loading checkpoint shards:  50%|█████     | 15/30 [01:12<01:12,  4.86s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [01:12<01:03,  4.50s/it]Loading checkpoint shards:  50%|█████     | 15/30 [01:14<01:14,  4.94s/it]Loading checkpoint shards:  60%|██████    | 18/30 [01:15<00:50,  4.18s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [01:16<01:08,  4.86s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [01:16<00:58,  4.50s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [01:17<01:07,  4.85s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [01:19<01:10,  5.00s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [01:19<00:47,  4.31s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [01:21<01:03,  4.87s/it]Loading checkpoint shards:  60%|██████    | 18/30 [01:21<00:55,  4.63s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [01:22<01:03,  4.90s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [01:23<00:42,  4.24s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [01:24<01:04,  4.96s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [01:26<00:51,  4.65s/it]Loading checkpoint shards:  60%|██████    | 18/30 [01:26<00:59,  4.92s/it]Loading checkpoint shards:  60%|██████    | 18/30 [01:27<00:59,  4.94s/it]Loading checkpoint shards:  70%|███████   | 21/30 [01:27<00:37,  4.16s/it]Loading checkpoint shards:  60%|██████    | 18/30 [01:29<01:00,  5.02s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [01:30<00:45,  4.58s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [01:31<00:54,  4.94s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [01:31<00:33,  4.14s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [01:32<00:54,  4.95s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [01:34<00:55,  5.01s/it]Loading checkpoint shards:  70%|███████   | 21/30 [01:35<00:41,  4.61s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [01:36<00:49,  4.92s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [01:36<00:29,  4.28s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [01:37<00:48,  4.84s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [01:39<00:49,  4.99s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [01:39<00:36,  4.55s/it]Loading checkpoint shards:  80%|████████  | 24/30 [01:41<00:25,  4.32s/it]Loading checkpoint shards:  70%|███████   | 21/30 [01:41<00:43,  4.82s/it]Loading checkpoint shards:  70%|███████   | 21/30 [01:41<00:43,  4.78s/it]Loading checkpoint shards:  70%|███████   | 21/30 [01:44<00:44,  4.90s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [01:44<00:32,  4.61s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [01:45<00:21,  4.25s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [01:46<00:38,  4.80s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [01:46<00:38,  4.78s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [01:49<00:39,  4.91s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [01:49<00:16,  4.22s/it]Loading checkpoint shards:  80%|████████  | 24/30 [01:49<00:28,  4.69s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [01:51<00:34,  4.91s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [01:51<00:34,  4.90s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [01:53<00:12,  4.25s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [01:54<00:23,  4.68s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [01:54<00:35,  5.06s/it]Loading checkpoint shards:  80%|████████  | 24/30 [01:56<00:30,  5.01s/it]Loading checkpoint shards:  80%|████████  | 24/30 [01:57<00:30,  5.03s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [01:58<00:08,  4.34s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [01:58<00:18,  4.64s/it]Loading checkpoint shards:  80%|████████  | 24/30 [01:59<00:30,  5.08s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [02:01<00:24,  4.93s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [02:01<00:24,  4.96s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [02:02<00:04,  4.38s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:03<00:00,  3.20s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:03<00:00,  4.10s/it]
Loading checkpoint shards:  90%|█████████ | 27/30 [02:03<00:13,  4.61s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [02:04<00:25,  5.01s/it]The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.
Loading checkpoint shards:  87%|████████▋ | 26/30 [02:05<00:19,  4.89s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [02:06<00:19,  4.91s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [02:08<00:09,  4.68s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [02:09<00:19,  4.92s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [02:10<00:14,  4.80s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [02:11<00:14,  4.84s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [02:12<00:04,  4.68s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:13<00:00,  3.40s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:13<00:00,  4.44s/it]
Loading checkpoint shards:  90%|█████████ | 27/30 [02:13<00:14,  4.85s/it]The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.
Loading checkpoint shards:  93%|█████████▎| 28/30 [02:15<00:09,  4.84s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [02:16<00:09,  4.87s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [02:18<00:09,  4.88s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [02:20<00:04,  4.86s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:21<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:21<00:00,  4.70s/it]
Loading checkpoint shards:  97%|█████████▋| 29/30 [02:21<00:04,  4.90s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:21<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:21<00:00,  4.73s/it]
[INFO|configuration_utils.py:939] 2025-11-13 22:13:24,699 >> loading configuration file /root/autodl-tmp/Llama-33-70B-Instruct/generation_config.json
[INFO|configuration_utils.py:986] 2025-11-13 22:13:24,699 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|dynamic_module_utils.py:423] 2025-11-13 22:13:24,700 >> Could not locate the custom_generate/generate.py inside /root/autodl-tmp/Llama-33-70B-Instruct.
[INFO|2025-11-13 22:13:24] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-11-13 22:13:24] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-11-13 22:13:24] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-11-13 22:13:24] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-11-13 22:13:24] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,up_proj,o_proj,k_proj,gate_proj,v_proj,down_proj
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.
Loading checkpoint shards:  97%|█████████▋| 29/30 [02:25<00:05,  5.29s/it][INFO|2025-11-13 22:13:28] llamafactory.model.loader:143 >> trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
[INFO|trainer.py:749] 2025-11-13 22:13:28,500 >> Using auto half precision backend
[WARNING|trainer.py:982] 2025-11-13 22:13:28,502 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.
Loading checkpoint shards: 100%|██████████| 30/30 [02:25<00:00,  3.87s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:25<00:00,  4.85s/it]
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.
[INFO|trainer.py:2519] 2025-11-13 22:13:32,254 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-11-13 22:13:32,254 >>   Num examples = 1,000
[INFO|trainer.py:2521] 2025-11-13 22:13:32,254 >>   Num Epochs = 20
[INFO|trainer.py:2522] 2025-11-13 22:13:32,254 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2525] 2025-11-13 22:13:32,254 >>   Total train batch size (w. parallel, distributed & accumulation) = 40
[INFO|trainer.py:2526] 2025-11-13 22:13:32,254 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2527] 2025-11-13 22:13:32,254 >>   Total optimization steps = 500
[INFO|trainer.py:2528] 2025-11-13 22:13:32,268 >>   Number of trainable parameters = 103,546,880
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:12<1:44:40, 12.59s/it]  0%|          | 2/500 [00:23<1:37:47, 11.78s/it]  1%|          | 3/500 [00:35<1:35:31, 11.53s/it]  1%|          | 4/500 [00:46<1:34:24, 11.42s/it]  1%|          | 5/500 [00:57<1:33:36, 11.35s/it]  1%|          | 6/500 [01:08<1:33:07, 11.31s/it]  1%|▏         | 7/500 [01:19<1:32:42, 11.28s/it]  2%|▏         | 8/500 [01:31<1:32:24, 11.27s/it]  2%|▏         | 9/500 [01:42<1:32:14, 11.27s/it]  2%|▏         | 10/500 [01:53<1:31:58, 11.26s/it]                                                  {'loss': 6.9958, 'grad_norm': 33.45831298828125, 'learning_rate': 1.8e-05, 'epoch': 0.4}
  2%|▏         | 10/500 [01:53<1:31:58, 11.26s/it]  2%|▏         | 11/500 [02:04<1:31:44, 11.26s/it]  2%|▏         | 12/500 [02:16<1:31:33, 11.26s/it]  3%|▎         | 13/500 [02:27<1:31:26, 11.27s/it]  3%|▎         | 14/500 [02:38<1:31:14, 11.27s/it]  3%|▎         | 15/500 [02:50<1:31:04, 11.27s/it]  3%|▎         | 16/500 [03:01<1:30:53, 11.27s/it]  3%|▎         | 17/500 [03:12<1:30:43, 11.27s/it]  4%|▎         | 18/500 [03:23<1:30:31, 11.27s/it]  4%|▍         | 19/500 [03:35<1:30:18, 11.26s/it]  4%|▍         | 20/500 [03:46<1:30:07, 11.27s/it]                                                  {'loss': 2.8473, 'grad_norm': 3.4119484424591064, 'learning_rate': 3.8e-05, 'epoch': 0.8}
  4%|▍         | 20/500 [03:46<1:30:07, 11.27s/it]  4%|▍         | 21/500 [03:57<1:29:59, 11.27s/it]  4%|▍         | 22/500 [04:08<1:29:48, 11.27s/it]  5%|▍         | 23/500 [04:20<1:29:42, 11.28s/it]  5%|▍         | 24/500 [04:31<1:29:31, 11.28s/it]  5%|▌         | 25/500 [04:42<1:29:37, 11.32s/it]  5%|▌         | 26/500 [04:54<1:30:25, 11.45s/it]  5%|▌         | 27/500 [05:05<1:29:53, 11.40s/it]  6%|▌         | 28/500 [05:17<1:29:25, 11.37s/it]  6%|▌         | 29/500 [05:28<1:29:01, 11.34s/it]  6%|▌         | 30/500 [05:39<1:28:42, 11.32s/it]                                                  {'loss': 1.243, 'grad_norm': 1.6324713230133057, 'learning_rate': 5.8e-05, 'epoch': 1.2}
  6%|▌         | 30/500 [05:39<1:28:42, 11.32s/it]  6%|▌         | 31/500 [05:51<1:28:26, 11.31s/it]  6%|▋         | 32/500 [06:02<1:28:11, 11.31s/it]  7%|▋         | 33/500 [06:13<1:28:00, 11.31s/it]  7%|▋         | 34/500 [06:25<1:27:49, 11.31s/it]  7%|▋         | 35/500 [06:36<1:27:35, 11.30s/it]  7%|▋         | 36/500 [06:47<1:27:23, 11.30s/it]  7%|▋         | 37/500 [06:58<1:27:12, 11.30s/it]  8%|▊         | 38/500 [07:10<1:27:01, 11.30s/it]  8%|▊         | 39/500 [07:21<1:26:51, 11.30s/it]  8%|▊         | 40/500 [07:32<1:26:35, 11.29s/it]                                                  {'loss': 1.1271, 'grad_norm': 2.299539804458618, 'learning_rate': 7.800000000000001e-05, 'epoch': 1.6}
  8%|▊         | 40/500 [07:32<1:26:35, 11.29s/it]  8%|▊         | 41/500 [07:44<1:26:22, 11.29s/it]  8%|▊         | 42/500 [07:55<1:26:12, 11.29s/it]  9%|▊         | 43/500 [08:06<1:26:00, 11.29s/it]  9%|▉         | 44/500 [08:17<1:25:45, 11.28s/it]  9%|▉         | 45/500 [08:29<1:25:35, 11.29s/it]  9%|▉         | 46/500 [08:40<1:25:25, 11.29s/it]  9%|▉         | 47/500 [08:51<1:25:13, 11.29s/it] 10%|▉         | 48/500 [09:03<1:25:05, 11.30s/it] 10%|▉         | 49/500 [09:14<1:24:50, 11.29s/it] 10%|█         | 50/500 [09:25<1:24:54, 11.32s/it]                                                  {'loss': 1.0237, 'grad_norm': 1.5836859941482544, 'learning_rate': 9.8e-05, 'epoch': 2.0}
 10%|█         | 50/500 [09:25<1:24:54, 11.32s/it] 10%|█         | 51/500 [09:37<1:25:42, 11.45s/it] 10%|█         | 52/500 [09:48<1:25:07, 11.40s/it] 11%|█         | 53/500 [10:00<1:24:41, 11.37s/it] 11%|█         | 54/500 [10:11<1:24:20, 11.35s/it] 11%|█         | 55/500 [10:22<1:23:59, 11.33s/it] 11%|█         | 56/500 [10:33<1:23:43, 11.31s/it] 11%|█▏        | 57/500 [10:45<1:23:29, 11.31s/it] 12%|█▏        | 58/500 [10:56<1:23:11, 11.29s/it] 12%|█▏        | 59/500 [11:07<1:22:58, 11.29s/it] 12%|█▏        | 60/500 [11:19<1:22:44, 11.28s/it]                                                  {'loss': 0.8528, 'grad_norm': 1.8672466278076172, 'learning_rate': 9.990133642141359e-05, 'epoch': 2.4}
 12%|█▏        | 60/500 [11:19<1:22:44, 11.28s/it] 12%|█▏        | 61/500 [11:30<1:22:33, 11.28s/it] 12%|█▏        | 62/500 [11:41<1:22:24, 11.29s/it] 13%|█▎        | 63/500 [11:52<1:22:15, 11.29s/it] 13%|█▎        | 64/500 [12:04<1:22:02, 11.29s/it] 13%|█▎        | 65/500 [12:15<1:21:50, 11.29s/it] 13%|█▎        | 66/500 [12:26<1:21:36, 11.28s/it] 13%|█▎        | 67/500 [12:38<1:21:28, 11.29s/it] 14%|█▎        | 68/500 [12:49<1:21:17, 11.29s/it] 14%|█▍        | 69/500 [13:00<1:21:07, 11.29s/it] 14%|█▍        | 70/500 [13:11<1:20:51, 11.28s/it]                                                  {'loss': 0.9154, 'grad_norm': 1.6828715801239014, 'learning_rate': 9.956077701257709e-05, 'epoch': 2.8}
 14%|█▍        | 70/500 [13:11<1:20:51, 11.28s/it] 14%|█▍        | 71/500 [13:23<1:20:48, 11.30s/it] 14%|█▍        | 72/500 [13:34<1:20:33, 11.29s/it] 15%|█▍        | 73/500 [13:45<1:20:21, 11.29s/it] 15%|█▍        | 74/500 [13:57<1:20:07, 11.28s/it] 15%|█▌        | 75/500 [14:08<1:20:09, 11.32s/it] 15%|█▌        | 76/500 [14:20<1:20:48, 11.43s/it] 15%|█▌        | 77/500 [14:31<1:20:18, 11.39s/it] 16%|█▌        | 78/500 [14:42<1:19:53, 11.36s/it] 16%|█▌        | 79/500 [14:54<1:19:30, 11.33s/it] 16%|█▌        | 80/500 [15:05<1:19:13, 11.32s/it]                                                  {'loss': 0.7624, 'grad_norm': 1.133802890777588, 'learning_rate': 9.89787624799672e-05, 'epoch': 3.2}
 16%|█▌        | 80/500 [15:05<1:19:13, 11.32s/it] 16%|█▌        | 81/500 [15:16<1:18:59, 11.31s/it] 16%|█▋        | 82/500 [15:27<1:18:43, 11.30s/it] 17%|█▋        | 83/500 [15:39<1:18:34, 11.31s/it] 17%|█▋        | 84/500 [15:50<1:18:24, 11.31s/it] 17%|█▋        | 85/500 [16:01<1:18:09, 11.30s/it] 17%|█▋        | 86/500 [16:13<1:17:57, 11.30s/it] 17%|█▋        | 87/500 [16:24<1:17:46, 11.30s/it] 18%|█▊        | 88/500 [16:35<1:17:31, 11.29s/it] 18%|█▊        | 89/500 [16:47<1:17:20, 11.29s/it] 18%|█▊        | 90/500 [16:58<1:17:07, 11.29s/it]                                                  {'loss': 0.6089, 'grad_norm': 1.5701526403427124, 'learning_rate': 9.815812833988291e-05, 'epoch': 3.6}
 18%|█▊        | 90/500 [16:58<1:17:07, 11.29s/it] 18%|█▊        | 91/500 [17:09<1:16:53, 11.28s/it] 18%|█▊        | 92/500 [17:20<1:16:45, 11.29s/it] 19%|█▊        | 93/500 [17:32<1:16:37, 11.30s/it] 19%|█▉        | 94/500 [17:43<1:16:22, 11.29s/it] 19%|█▉        | 95/500 [17:54<1:16:11, 11.29s/it] 19%|█▉        | 96/500 [18:06<1:16:02, 11.29s/it] 19%|█▉        | 97/500 [18:17<1:15:48, 11.29s/it] 20%|█▉        | 98/500 [18:28<1:15:36, 11.28s/it] 20%|█▉        | 99/500 [18:39<1:15:30, 11.30s/it] 20%|██        | 100/500 [18:51<1:15:29, 11.32s/it]                                                   {'loss': 0.6696, 'grad_norm': 1.4049570560455322, 'learning_rate': 9.710287263936484e-05, 'epoch': 4.0}
 20%|██        | 100/500 [18:51<1:15:29, 11.32s/it] 20%|██        | 101/500 [19:03<1:16:03, 11.44s/it] 20%|██        | 102/500 [19:14<1:15:33, 11.39s/it] 21%|██        | 103/500 [19:25<1:15:07, 11.35s/it] 21%|██        | 104/500 [19:37<1:15:21, 11.42s/it] 21%|██        | 105/500 [19:48<1:14:54, 11.38s/it] 21%|██        | 106/500 [19:59<1:15:04, 11.43s/it] 21%|██▏       | 107/500 [20:11<1:15:47, 11.57s/it] 22%|██▏       | 108/500 [20:23<1:15:02, 11.49s/it] 22%|██▏       | 109/500 [20:34<1:14:26, 11.42s/it] 22%|██▏       | 110/500 [20:46<1:14:35, 11.48s/it]                                                   {'loss': 0.4204, 'grad_norm': 2.9074535369873047, 'learning_rate': 9.581813647811198e-05, 'epoch': 4.4}
 22%|██▏       | 110/500 [20:46<1:14:35, 11.48s/it] 22%|██▏       | 111/500 [20:57<1:14:03, 11.42s/it] 22%|██▏       | 112/500 [21:08<1:13:34, 11.38s/it] 23%|██▎       | 113/500 [21:19<1:13:12, 11.35s/it] 23%|██▎       | 114/500 [21:31<1:12:54, 11.33s/it] 23%|██▎       | 115/500 [21:42<1:12:35, 11.31s/it] 23%|██▎       | 116/500 [21:53<1:12:20, 11.30s/it] 23%|██▎       | 117/500 [22:05<1:12:05, 11.29s/it] 24%|██▎       | 118/500 [22:16<1:11:52, 11.29s/it] 24%|██▍       | 119/500 [22:27<1:11:41, 11.29s/it] 24%|██▍       | 120/500 [22:38<1:11:32, 11.30s/it]                                                   {'loss': 0.4362, 'grad_norm': 2.213644504547119, 'learning_rate': 9.431017896156074e-05, 'epoch': 4.8}
 24%|██▍       | 120/500 [22:38<1:11:32, 11.30s/it] 24%|██▍       | 121/500 [22:50<1:11:19, 11.29s/it] 24%|██▍       | 122/500 [23:01<1:11:07, 11.29s/it] 25%|██▍       | 123/500 [23:12<1:10:57, 11.29s/it] 25%|██▍       | 124/500 [23:24<1:10:44, 11.29s/it] 25%|██▌       | 125/500 [23:35<1:10:45, 11.32s/it] 25%|██▌       | 126/500 [23:47<1:11:20, 11.45s/it] 25%|██▌       | 127/500 [23:58<1:10:49, 11.39s/it] 26%|██▌       | 128/500 [24:09<1:10:22, 11.35s/it] 26%|██▌       | 129/500 [24:20<1:10:03, 11.33s/it] 26%|██▌       | 130/500 [24:32<1:09:46, 11.31s/it]                                                   {'loss': 0.3118, 'grad_norm': 2.286285400390625, 'learning_rate': 9.258634670715238e-05, 'epoch': 5.2}
 26%|██▌       | 130/500 [24:32<1:09:46, 11.31s/it] 26%|██▌       | 131/500 [24:43<1:09:32, 11.31s/it] 26%|██▋       | 132/500 [24:54<1:09:20, 11.30s/it] 27%|██▋       | 133/500 [25:06<1:09:06, 11.30s/it] 27%|██▋       | 134/500 [25:17<1:08:54, 11.30s/it] 27%|██▋       | 135/500 [25:28<1:08:46, 11.30s/it] 27%|██▋       | 136/500 [25:40<1:08:31, 11.30s/it] 27%|██▋       | 137/500 [25:51<1:08:22, 11.30s/it] 28%|██▊       | 138/500 [26:02<1:08:09, 11.30s/it] 28%|██▊       | 139/500 [26:13<1:07:55, 11.29s/it] 28%|██▊       | 140/500 [26:25<1:07:45, 11.29s/it]                                                   {'loss': 0.1979, 'grad_norm': 3.630727767944336, 'learning_rate': 9.065503805235138e-05, 'epoch': 5.6}
 28%|██▊       | 140/500 [26:25<1:07:45, 11.29s/it] 28%|██▊       | 141/500 [26:36<1:07:30, 11.28s/it] 28%|██▊       | 142/500 [26:47<1:07:20, 11.29s/it] 29%|██▊       | 143/500 [26:59<1:07:07, 11.28s/it] 29%|██▉       | 144/500 [27:10<1:06:55, 11.28s/it] 29%|██▉       | 145/500 [27:21<1:06:48, 11.29s/it] 29%|██▉       | 146/500 [27:32<1:06:36, 11.29s/it] 29%|██▉       | 147/500 [27:44<1:06:23, 11.28s/it] 30%|██▉       | 148/500 [27:55<1:06:13, 11.29s/it] 30%|██▉       | 149/500 [28:06<1:06:01, 11.29s/it] 30%|███       | 150/500 [28:18<1:06:03, 11.32s/it]                                                   {'loss': 0.1935, 'grad_norm': 3.512479305267334, 'learning_rate': 8.852566213878947e-05, 'epoch': 6.0}
 30%|███       | 150/500 [28:18<1:06:03, 11.32s/it] 30%|███       | 151/500 [28:29<1:06:36, 11.45s/it] 30%|███       | 152/500 [28:41<1:06:07, 11.40s/it] 31%|███       | 153/500 [28:52<1:05:43, 11.36s/it] 31%|███       | 154/500 [29:03<1:05:22, 11.34s/it] 31%|███       | 155/500 [29:15<1:05:04, 11.32s/it] 31%|███       | 156/500 [29:26<1:04:50, 11.31s/it] 31%|███▏      | 157/500 [29:37<1:04:38, 11.31s/it] 32%|███▏      | 158/500 [29:48<1:04:23, 11.30s/it] 32%|███▏      | 159/500 [30:00<1:04:09, 11.29s/it] 32%|███▏      | 160/500 [30:11<1:03:59, 11.29s/it]                                                   {'loss': 0.0754, 'grad_norm': 3.1039507389068604, 'learning_rate': 8.620859307187339e-05, 'epoch': 6.4}
 32%|███▏      | 160/500 [30:11<1:03:59, 11.29s/it] 32%|███▏      | 161/500 [30:22<1:03:47, 11.29s/it] 32%|███▏      | 162/500 [30:33<1:03:33, 11.28s/it] 33%|███▎      | 163/500 [30:45<1:03:22, 11.28s/it] 33%|███▎      | 164/500 [30:56<1:03:14, 11.29s/it] 33%|███▎      | 165/500 [31:07<1:02:59, 11.28s/it] 33%|███▎      | 166/500 [31:19<1:02:49, 11.29s/it] 33%|███▎      | 167/500 [31:30<1:02:36, 11.28s/it] 34%|███▎      | 168/500 [31:41<1:02:23, 11.28s/it] 34%|███▍      | 169/500 [31:52<1:02:14, 11.28s/it] 34%|███▍      | 170/500 [32:04<1:02:03, 11.28s/it]                                                   {'loss': 0.1242, 'grad_norm': 3.5898234844207764, 'learning_rate': 8.371511937918616e-05, 'epoch': 6.8}
 34%|███▍      | 170/500 [32:04<1:02:03, 11.28s/it] 34%|███▍      | 171/500 [32:15<1:01:51, 11.28s/it] 34%|███▍      | 172/500 [32:26<1:01:39, 11.28s/it] 35%|███▍      | 173/500 [32:38<1:01:30, 11.28s/it] 35%|███▍      | 174/500 [32:49<1:01:16, 11.28s/it] 35%|███▌      | 175/500 [33:00<1:01:22, 11.33s/it] 35%|███▌      | 176/500 [33:12<1:01:45, 11.44s/it] 35%|███▌      | 177/500 [33:23<1:01:18, 11.39s/it] 36%|███▌      | 178/500 [33:35<1:00:56, 11.36s/it] 36%|███▌      | 179/500 [33:46<1:00:38, 11.33s/it] 36%|███▌      | 180/500 [33:57<1:00:22, 11.32s/it]                                                   {'loss': 0.0768, 'grad_norm': 0.8683528900146484, 'learning_rate': 8.105738901391552e-05, 'epoch': 7.2}
 36%|███▌      | 180/500 [33:57<1:00:22, 11.32s/it] 36%|███▌      | 181/500 [34:08<1:00:07, 11.31s/it] 36%|███▋      | 182/500 [34:20<59:53, 11.30s/it]   37%|███▋      | 183/500 [34:31<59:39, 11.29s/it] 37%|███▋      | 184/500 [34:42<59:27, 11.29s/it] 37%|███▋      | 185/500 [34:54<59:15, 11.29s/it] 37%|███▋      | 186/500 [35:05<59:03, 11.29s/it] 37%|███▋      | 187/500 [35:16<58:53, 11.29s/it] 38%|███▊      | 188/500 [35:27<58:42, 11.29s/it] 38%|███▊      | 189/500 [35:39<58:29, 11.29s/it] 38%|███▊      | 190/500 [35:50<58:20, 11.29s/it]                                                 {'loss': 0.0353, 'grad_norm': 1.2546327114105225, 'learning_rate': 7.82483501712469e-05, 'epoch': 7.6}
 38%|███▊      | 190/500 [35:50<58:20, 11.29s/it] 38%|███▊      | 191/500 [36:01<58:08, 11.29s/it] 38%|███▊      | 192/500 [36:13<57:56, 11.29s/it] 39%|███▊      | 193/500 [36:24<57:46, 11.29s/it] 39%|███▉      | 194/500 [36:35<57:35, 11.29s/it] 39%|███▉      | 195/500 [36:46<57:21, 11.28s/it] 39%|███▉      | 196/500 [36:58<57:09, 11.28s/it] 39%|███▉      | 197/500 [37:09<56:58, 11.28s/it] 40%|███▉      | 198/500 [37:20<56:46, 11.28s/it] 40%|███▉      | 199/500 [37:32<56:34, 11.28s/it] 40%|████      | 200/500 [37:43<56:33, 11.31s/it]                                                 {'loss': 0.0669, 'grad_norm': 2.867703914642334, 'learning_rate': 7.530168820605818e-05, 'epoch': 8.0}
 40%|████      | 200/500 [37:43<56:33, 11.31s/it] 40%|████      | 201/500 [37:55<56:57, 11.43s/it] 40%|████      | 202/500 [38:06<56:34, 11.39s/it] 41%|████      | 203/500 [38:17<56:12, 11.36s/it] 41%|████      | 204/500 [38:28<55:54, 11.33s/it] 41%|████      | 205/500 [38:40<55:41, 11.33s/it] 41%|████      | 206/500 [38:51<55:25, 11.31s/it] 41%|████▏     | 207/500 [39:02<55:12, 11.30s/it] 42%|████▏     | 208/500 [39:14<55:01, 11.30s/it] 42%|████▏     | 209/500 [39:25<54:49, 11.30s/it] 42%|████▏     | 210/500 [39:36<54:37, 11.30s/it]                                                 {'loss': 0.045, 'grad_norm': 1.2059427499771118, 'learning_rate': 7.223175895924638e-05, 'epoch': 8.4}
 42%|████▏     | 210/500 [39:36<54:37, 11.30s/it] 42%|████▏     | 211/500 [39:48<54:49, 11.38s/it] 42%|████▏     | 212/500 [39:59<54:28, 11.35s/it] 43%|████▎     | 213/500 [40:10<54:11, 11.33s/it] 43%|████▎     | 214/500 [40:22<53:55, 11.31s/it] 43%|████▎     | 215/500 [40:33<53:43, 11.31s/it] 43%|████▎     | 216/500 [40:45<53:54, 11.39s/it] 43%|████▎     | 217/500 [40:56<53:58, 11.45s/it] 44%|████▎     | 218/500 [41:08<54:07, 11.52s/it] 44%|████▍     | 219/500 [41:19<53:36, 11.45s/it] 44%|████▍     | 220/500 [41:30<53:12, 11.40s/it]                                                 {'loss': 0.0295, 'grad_norm': 0.9153308868408203, 'learning_rate': 6.905351881751372e-05, 'epoch': 8.8}
 44%|████▍     | 220/500 [41:30<53:12, 11.40s/it] 44%|████▍     | 221/500 [41:42<52:50, 11.36s/it] 44%|████▍     | 222/500 [41:53<52:35, 11.35s/it] 45%|████▍     | 223/500 [42:05<52:50, 11.45s/it] 45%|████▍     | 224/500 [42:16<52:23, 11.39s/it] 45%|████▌     | 225/500 [42:27<52:11, 11.39s/it] 45%|████▌     | 226/500 [42:39<52:29, 11.50s/it] 45%|████▌     | 227/500 [42:50<52:00, 11.43s/it] 46%|████▌     | 228/500 [43:02<51:36, 11.39s/it] 46%|████▌     | 229/500 [43:13<51:16, 11.35s/it] 46%|████▌     | 230/500 [43:24<51:00, 11.33s/it]                                                 {'loss': 0.0334, 'grad_norm': 1.293434739112854, 'learning_rate': 6.578245184735513e-05, 'epoch': 9.2}
 46%|████▌     | 230/500 [43:24<51:00, 11.33s/it] 46%|████▌     | 231/500 [43:35<50:44, 11.32s/it] 46%|████▋     | 232/500 [43:47<50:32, 11.32s/it] 47%|████▋     | 233/500 [43:58<50:17, 11.30s/it] 47%|████▋     | 234/500 [44:09<50:04, 11.30s/it] 47%|████▋     | 235/500 [44:21<49:53, 11.29s/it] 47%|████▋     | 236/500 [44:32<49:39, 11.28s/it] 47%|████▋     | 237/500 [44:43<49:28, 11.29s/it] 48%|████▊     | 238/500 [44:54<49:18, 11.29s/it] 48%|████▊     | 239/500 [45:06<49:05, 11.28s/it] 48%|████▊     | 240/500 [45:17<48:52, 11.28s/it]                                                 {'loss': 0.0174, 'grad_norm': 0.4990594983100891, 'learning_rate': 6.243449435824276e-05, 'epoch': 9.6}
 48%|████▊     | 240/500 [45:17<48:52, 11.28s/it] 48%|████▊     | 241/500 [45:28<48:42, 11.28s/it] 48%|████▊     | 242/500 [45:40<48:29, 11.28s/it] 49%|████▊     | 243/500 [45:51<48:22, 11.29s/it] 49%|████▉     | 244/500 [46:02<48:10, 11.29s/it] 49%|████▉     | 245/500 [46:13<47:57, 11.28s/it] 49%|████▉     | 246/500 [46:25<47:45, 11.28s/it] 49%|████▉     | 247/500 [46:36<47:33, 11.28s/it] 50%|████▉     | 248/500 [46:47<47:24, 11.29s/it] 50%|████▉     | 249/500 [46:59<47:11, 11.28s/it] 50%|█████     | 250/500 [47:10<47:07, 11.31s/it]                                                 {'loss': 0.0315, 'grad_norm': 0.3472166955471039, 'learning_rate': 5.902595726252801e-05, 'epoch': 10.0}
 50%|█████     | 250/500 [47:10<47:07, 11.31s/it] 50%|█████     | 251/500 [47:22<47:29, 11.45s/it] 50%|█████     | 252/500 [47:33<47:06, 11.40s/it] 51%|█████     | 253/500 [47:44<46:45, 11.36s/it] 51%|█████     | 254/500 [47:56<46:29, 11.34s/it] 51%|█████     | 255/500 [48:07<46:17, 11.34s/it] 51%|█████     | 256/500 [48:18<46:01, 11.32s/it] 51%|█████▏    | 257/500 [48:29<45:48, 11.31s/it] 52%|█████▏    | 258/500 [48:41<45:34, 11.30s/it] 52%|█████▏    | 259/500 [48:52<45:23, 11.30s/it] 52%|█████▏    | 260/500 [49:03<45:08, 11.29s/it]                                                 {'loss': 0.0258, 'grad_norm': 0.1916096955537796, 'learning_rate': 5.557344661031627e-05, 'epoch': 10.4}
 52%|█████▏    | 260/500 [49:03<45:08, 11.29s/it] 52%|█████▏    | 261/500 [49:15<44:58, 11.29s/it] 52%|█████▏    | 262/500 [49:26<44:46, 11.29s/it] 53%|█████▎    | 263/500 [49:37<44:35, 11.29s/it] 53%|█████▎    | 264/500 [49:48<44:24, 11.29s/it] 53%|█████▎    | 265/500 [50:00<44:12, 11.29s/it] 53%|█████▎    | 266/500 [50:11<44:00, 11.29s/it] 53%|█████▎    | 267/500 [50:22<43:50, 11.29s/it] 54%|█████▎    | 268/500 [50:34<43:37, 11.28s/it] 54%|█████▍    | 269/500 [50:45<43:26, 11.28s/it] 54%|█████▍    | 270/500 [50:56<43:14, 11.28s/it]                                                 {'loss': 0.0099, 'grad_norm': 0.11424902081489563, 'learning_rate': 5.209378268645998e-05, 'epoch': 10.8}
 54%|█████▍    | 270/500 [50:56<43:14, 11.28s/it] 54%|█████▍    | 271/500 [51:07<43:02, 11.28s/it] 54%|█████▍    | 272/500 [51:19<42:51, 11.28s/it] 55%|█████▍    | 273/500 [51:30<42:41, 11.29s/it] 55%|█████▍    | 274/500 [51:41<42:29, 11.28s/it] 55%|█████▌    | 275/500 [51:53<42:27, 11.32s/it] 55%|█████▌    | 276/500 [52:04<42:42, 11.44s/it] 55%|█████▌    | 277/500 [52:16<42:18, 11.38s/it] 56%|█████▌    | 278/500 [52:27<42:00, 11.35s/it] 56%|█████▌    | 279/500 [52:38<41:44, 11.33s/it] 56%|█████▌    | 280/500 [52:49<41:30, 11.32s/it]                                                 {'loss': 0.0088, 'grad_norm': 0.019844138994812965, 'learning_rate': 4.860391806382157e-05, 'epoch': 11.2}
 56%|█████▌    | 280/500 [52:49<41:30, 11.32s/it] 56%|█████▌    | 281/500 [53:01<41:18, 11.32s/it] 56%|█████▋    | 282/500 [53:12<41:05, 11.31s/it] 57%|█████▋    | 283/500 [53:23<40:53, 11.31s/it] 57%|█████▋    | 284/500 [53:35<40:39, 11.29s/it] 57%|█████▋    | 285/500 [53:46<40:28, 11.29s/it] 57%|█████▋    | 286/500 [53:57<40:15, 11.29s/it] 57%|█████▋    | 287/500 [54:09<40:04, 11.29s/it] 58%|█████▊    | 288/500 [54:20<39:54, 11.29s/it] 58%|█████▊    | 289/500 [54:31<39:41, 11.29s/it] 58%|█████▊    | 290/500 [54:42<39:30, 11.29s/it]                                                 {'loss': 0.0023, 'grad_norm': 1.0002810955047607, 'learning_rate': 4.512085501204253e-05, 'epoch': 11.6}
 58%|█████▊    | 290/500 [54:42<39:30, 11.29s/it] 58%|█████▊    | 291/500 [54:54<39:18, 11.29s/it] 58%|█████▊    | 292/500 [55:05<39:06, 11.28s/it] 59%|█████▊    | 293/500 [55:16<38:55, 11.28s/it] 59%|█████▉    | 294/500 [55:27<38:43, 11.28s/it] 59%|█████▉    | 295/500 [55:39<38:32, 11.28s/it] 59%|█████▉    | 296/500 [55:50<38:21, 11.28s/it] 59%|█████▉    | 297/500 [56:01<38:10, 11.28s/it] 60%|█████▉    | 298/500 [56:13<37:57, 11.28s/it] 60%|█████▉    | 299/500 [56:24<37:48, 11.29s/it] 60%|██████    | 300/500 [56:35<37:44, 11.32s/it]                                                 {'loss': 0.0046, 'grad_norm': 0.07766636461019516, 'learning_rate': 4.166156266419489e-05, 'epoch': 12.0}
 60%|██████    | 300/500 [56:35<37:44, 11.32s/it] 60%|██████    | 301/500 [56:47<37:57, 11.44s/it] 60%|██████    | 302/500 [56:58<37:37, 11.40s/it] 61%|██████    | 303/500 [57:10<37:18, 11.36s/it] 61%|██████    | 304/500 [57:21<37:02, 11.34s/it] 61%|██████    | 305/500 [57:32<36:47, 11.32s/it] 61%|██████    | 306/500 [57:43<36:33, 11.30s/it] 61%|██████▏   | 307/500 [57:55<36:19, 11.29s/it] 62%|██████▏   | 308/500 [58:06<36:08, 11.30s/it] 62%|██████▏   | 309/500 [58:17<35:57, 11.30s/it] 62%|██████▏   | 310/500 [58:29<35:45, 11.29s/it]                                                 {'loss': 0.0012, 'grad_norm': 0.7199171185493469, 'learning_rate': 3.82428943448705e-05, 'epoch': 12.4}
 62%|██████▏   | 310/500 [58:29<35:45, 11.29s/it] 62%|██████▏   | 311/500 [58:40<35:34, 11.29s/it] 62%|██████▏   | 312/500 [58:51<35:21, 11.29s/it] 63%|██████▎   | 313/500 [59:02<35:10, 11.28s/it] 63%|██████▎   | 314/500 [59:14<35:00, 11.29s/it] 63%|██████▎   | 315/500 [59:25<34:47, 11.28s/it] 63%|██████▎   | 316/500 [59:36<34:35, 11.28s/it] 63%|██████▎   | 317/500 [59:48<34:25, 11.28s/it] 64%|██████▎   | 318/500 [59:59<34:13, 11.28s/it] 64%|██████▍   | 319/500 [1:00:10<34:19, 11.38s/it] 64%|██████▍   | 320/500 [1:00:22<34:02, 11.35s/it]                                                   {'loss': 0.0045, 'grad_norm': 0.012324349023401737, 'learning_rate': 3.488150546247778e-05, 'epoch': 12.8}
 64%|██████▍   | 320/500 [1:00:22<34:02, 11.35s/it] 64%|██████▍   | 321/500 [1:00:33<33:47, 11.33s/it] 64%|██████▍   | 322/500 [1:00:44<33:32, 11.31s/it] 65%|██████▍   | 323/500 [1:00:56<33:20, 11.30s/it] 65%|██████▍   | 324/500 [1:01:07<33:06, 11.29s/it] 65%|██████▌   | 325/500 [1:01:18<33:00, 11.32s/it] 65%|██████▌   | 326/500 [1:01:30<33:30, 11.56s/it] 65%|██████▌   | 327/500 [1:01:42<33:04, 11.47s/it] 66%|██████▌   | 328/500 [1:01:53<32:43, 11.41s/it] 66%|██████▌   | 329/500 [1:02:04<32:40, 11.47s/it] 66%|██████▌   | 330/500 [1:02:16<32:18, 11.40s/it]                                                   {'loss': 0.0005, 'grad_norm': 0.008611892350018024, 'learning_rate': 3.1593772365766105e-05, 'epoch': 13.2}
 66%|██████▌   | 330/500 [1:02:16<32:18, 11.40s/it] 66%|██████▌   | 331/500 [1:02:27<32:01, 11.37s/it] 66%|██████▋   | 332/500 [1:02:38<31:46, 11.35s/it] 67%|██████▋   | 333/500 [1:02:50<31:42, 11.39s/it] 67%|██████▋   | 334/500 [1:03:01<31:26, 11.36s/it] 67%|██████▋   | 335/500 [1:03:12<31:11, 11.34s/it] 67%|██████▋   | 336/500 [1:03:24<30:56, 11.32s/it] 67%|██████▋   | 337/500 [1:03:35<30:43, 11.31s/it] 68%|██████▊   | 338/500 [1:03:46<30:30, 11.30s/it] 68%|██████▊   | 339/500 [1:03:58<30:18, 11.30s/it] 68%|██████▊   | 340/500 [1:04:09<30:07, 11.30s/it]                                                   {'loss': 0.0016, 'grad_norm': 0.007037756033241749, 'learning_rate': 2.8395712559900877e-05, 'epoch': 13.6}
 68%|██████▊   | 340/500 [1:04:09<30:07, 11.30s/it] 68%|██████▊   | 341/500 [1:04:20<29:56, 11.30s/it] 68%|██████▊   | 342/500 [1:04:31<29:43, 11.29s/it] 69%|██████▊   | 343/500 [1:04:43<29:31, 11.28s/it] 69%|██████▉   | 344/500 [1:04:54<29:20, 11.28s/it] 69%|██████▉   | 345/500 [1:05:05<29:08, 11.28s/it] 69%|██████▉   | 346/500 [1:05:16<28:56, 11.28s/it] 69%|██████▉   | 347/500 [1:05:28<28:45, 11.28s/it] 70%|██████▉   | 348/500 [1:05:39<28:34, 11.28s/it] 70%|██████▉   | 349/500 [1:05:50<28:23, 11.28s/it] 70%|███████   | 350/500 [1:06:02<28:16, 11.31s/it]                                                   {'loss': 0.0022, 'grad_norm': 0.005911584943532944, 'learning_rate': 2.5302906670788462e-05, 'epoch': 14.0}
 70%|███████   | 350/500 [1:06:02<28:16, 11.31s/it] 70%|███████   | 351/500 [1:06:13<28:24, 11.44s/it] 70%|███████   | 352/500 [1:06:25<28:06, 11.39s/it] 71%|███████   | 353/500 [1:06:36<27:49, 11.36s/it] 71%|███████   | 354/500 [1:06:47<27:35, 11.34s/it] 71%|███████   | 355/500 [1:06:59<27:21, 11.32s/it] 71%|███████   | 356/500 [1:07:10<27:08, 11.31s/it] 71%|███████▏  | 357/500 [1:07:21<26:55, 11.30s/it] 72%|███████▏  | 358/500 [1:07:32<26:43, 11.29s/it] 72%|███████▏  | 359/500 [1:07:44<26:33, 11.30s/it] 72%|███████▏  | 360/500 [1:07:55<26:21, 11.29s/it]                                                   {'loss': 0.0003, 'grad_norm': 0.008999744430184364, 'learning_rate': 2.23304225378328e-05, 'epoch': 14.4}
 72%|███████▏  | 360/500 [1:07:55<26:21, 11.29s/it] 72%|███████▏  | 361/500 [1:08:06<26:09, 11.29s/it] 72%|███████▏  | 362/500 [1:08:18<25:58, 11.29s/it] 73%|███████▎  | 363/500 [1:08:29<25:46, 11.29s/it] 73%|███████▎  | 364/500 [1:08:40<25:36, 11.29s/it] 73%|███████▎  | 365/500 [1:08:51<25:24, 11.29s/it] 73%|███████▎  | 366/500 [1:09:03<25:12, 11.29s/it] 73%|███████▎  | 367/500 [1:09:14<25:01, 11.29s/it] 74%|███████▎  | 368/500 [1:09:25<24:49, 11.29s/it] 74%|███████▍  | 369/500 [1:09:37<24:37, 11.28s/it] 74%|███████▍  | 370/500 [1:09:48<24:27, 11.29s/it]                                                   {'loss': 0.0027, 'grad_norm': 0.1648993194103241, 'learning_rate': 1.9492741804936622e-05, 'epoch': 14.8}
 74%|███████▍  | 370/500 [1:09:48<24:27, 11.29s/it] 74%|███████▍  | 371/500 [1:09:59<24:16, 11.29s/it] 74%|███████▍  | 372/500 [1:10:10<24:05, 11.29s/it] 75%|███████▍  | 373/500 [1:10:22<23:54, 11.29s/it] 75%|███████▍  | 374/500 [1:10:33<23:42, 11.29s/it] 75%|███████▌  | 375/500 [1:10:44<23:34, 11.31s/it] 75%|███████▌  | 376/500 [1:10:56<23:37, 11.44s/it] 75%|███████▌  | 377/500 [1:11:07<23:20, 11.39s/it] 76%|███████▌  | 378/500 [1:11:19<23:06, 11.37s/it] 76%|███████▌  | 379/500 [1:11:30<22:52, 11.34s/it] 76%|███████▌  | 380/500 [1:11:41<22:38, 11.32s/it]                                                   {'loss': 0.0009, 'grad_norm': 0.003918676637113094, 'learning_rate': 1.680368936738792e-05, 'epoch': 15.2}
 76%|███████▌  | 380/500 [1:11:41<22:38, 11.32s/it] 76%|███████▌  | 381/500 [1:11:53<22:26, 11.32s/it] 76%|███████▋  | 382/500 [1:12:04<22:14, 11.31s/it] 77%|███████▋  | 383/500 [1:12:15<22:02, 11.30s/it] 77%|███████▋  | 384/500 [1:12:26<21:50, 11.29s/it] 77%|███████▋  | 385/500 [1:12:38<21:37, 11.29s/it] 77%|███████▋  | 386/500 [1:12:49<21:26, 11.29s/it] 77%|███████▋  | 387/500 [1:13:00<21:13, 11.27s/it] 78%|███████▊  | 388/500 [1:13:12<21:03, 11.28s/it] 78%|███████▊  | 389/500 [1:13:23<20:51, 11.28s/it] 78%|███████▊  | 390/500 [1:13:34<20:40, 11.27s/it]                                                   {'loss': 0.0008, 'grad_norm': 0.0033539850264787674, 'learning_rate': 1.4276366018359844e-05, 'epoch': 15.6}
 78%|███████▊  | 390/500 [1:13:34<20:40, 11.27s/it] 78%|███████▊  | 391/500 [1:13:45<20:29, 11.28s/it] 78%|███████▊  | 392/500 [1:13:57<20:17, 11.27s/it] 79%|███████▊  | 393/500 [1:14:08<20:06, 11.27s/it] 79%|███████▉  | 394/500 [1:14:19<19:55, 11.28s/it] 79%|███████▉  | 395/500 [1:14:30<19:43, 11.27s/it] 79%|███████▉  | 396/500 [1:14:42<19:32, 11.27s/it] 79%|███████▉  | 397/500 [1:14:53<19:21, 11.28s/it] 80%|███████▉  | 398/500 [1:15:04<19:10, 11.28s/it] 80%|███████▉  | 399/500 [1:15:16<18:58, 11.27s/it] 80%|████████  | 400/500 [1:15:27<18:51, 11.31s/it]                                                   {'loss': 0.0012, 'grad_norm': 0.1717585325241089, 'learning_rate': 1.1923084623163172e-05, 'epoch': 16.0}
 80%|████████  | 400/500 [1:15:27<18:51, 11.31s/it] 80%|████████  | 401/500 [1:15:39<18:52, 11.44s/it] 80%|████████  | 402/500 [1:15:50<18:35, 11.38s/it] 81%|████████  | 403/500 [1:16:01<18:21, 11.36s/it] 81%|████████  | 404/500 [1:16:13<18:08, 11.33s/it] 81%|████████  | 405/500 [1:16:24<17:54, 11.31s/it] 81%|████████  | 406/500 [1:16:35<17:43, 11.31s/it] 81%|████████▏ | 407/500 [1:16:46<17:31, 11.30s/it] 82%|████████▏ | 408/500 [1:16:58<17:18, 11.29s/it] 82%|████████▏ | 409/500 [1:17:09<17:07, 11.29s/it] 82%|████████▏ | 410/500 [1:17:20<16:55, 11.28s/it]                                                   {'loss': 0.0012, 'grad_norm': 0.14182789623737335, 'learning_rate': 9.755310132204298e-06, 'epoch': 16.4}
 82%|████████▏ | 410/500 [1:17:20<16:55, 11.28s/it] 82%|████████▏ | 411/500 [1:17:31<16:43, 11.28s/it] 82%|████████▏ | 412/500 [1:17:43<16:33, 11.29s/it] 83%|████████▎ | 413/500 [1:17:54<16:21, 11.28s/it] 83%|████████▎ | 414/500 [1:18:05<16:10, 11.28s/it] 83%|████████▎ | 415/500 [1:18:17<15:59, 11.28s/it] 83%|████████▎ | 416/500 [1:18:28<15:47, 11.28s/it] 83%|████████▎ | 417/500 [1:18:39<15:36, 11.28s/it] 84%|████████▎ | 418/500 [1:18:50<15:25, 11.28s/it] 84%|████████▍ | 419/500 [1:19:02<15:13, 11.28s/it] 84%|████████▍ | 420/500 [1:19:13<15:02, 11.28s/it]                                                   {'loss': 0.0017, 'grad_norm': 0.004165271762758493, 'learning_rate': 7.783603724899257e-06, 'epoch': 16.8}
 84%|████████▍ | 420/500 [1:19:13<15:02, 11.28s/it] 84%|████████▍ | 421/500 [1:19:24<14:51, 11.28s/it] 84%|████████▍ | 422/500 [1:19:36<14:40, 11.28s/it] 85%|████████▍ | 423/500 [1:19:47<14:28, 11.28s/it] 85%|████████▍ | 424/500 [1:19:58<14:18, 11.30s/it] 85%|████████▌ | 425/500 [1:20:10<14:09, 11.32s/it] 85%|████████▌ | 426/500 [1:20:21<14:07, 11.45s/it] 85%|████████▌ | 427/500 [1:20:33<13:58, 11.49s/it] 86%|████████▌ | 428/500 [1:20:44<13:42, 11.42s/it] 86%|████████▌ | 429/500 [1:20:55<13:28, 11.38s/it] 86%|████████▌ | 430/500 [1:21:07<13:14, 11.36s/it]                                                   {'loss': 0.0001, 'grad_norm': 0.003707870142534375, 'learning_rate': 6.017571356669183e-06, 'epoch': 17.2}
 86%|████████▌ | 430/500 [1:21:07<13:14, 11.36s/it] 86%|████████▌ | 431/500 [1:21:18<13:01, 11.33s/it] 86%|████████▋ | 432/500 [1:21:29<12:49, 11.32s/it] 87%|████████▋ | 433/500 [1:21:41<12:37, 11.31s/it] 87%|████████▋ | 434/500 [1:21:52<12:25, 11.30s/it] 87%|████████▋ | 435/500 [1:22:03<12:14, 11.29s/it] 87%|████████▋ | 436/500 [1:22:14<12:03, 11.30s/it] 87%|████████▋ | 437/500 [1:22:26<11:51, 11.29s/it] 88%|████████▊ | 438/500 [1:22:37<11:39, 11.29s/it] 88%|████████▊ | 439/500 [1:22:49<11:33, 11.37s/it] 88%|████████▊ | 440/500 [1:23:00<11:21, 11.35s/it]                                                   {'loss': 0.0019, 'grad_norm': 0.005085993558168411, 'learning_rate': 4.465816959691149e-06, 'epoch': 17.6}
 88%|████████▊ | 440/500 [1:23:00<11:21, 11.35s/it] 88%|████████▊ | 441/500 [1:23:12<11:19, 11.52s/it] 88%|████████▊ | 442/500 [1:23:23<11:03, 11.45s/it] 89%|████████▊ | 443/500 [1:23:34<10:49, 11.40s/it] 89%|████████▉ | 444/500 [1:23:46<10:36, 11.36s/it] 89%|████████▉ | 445/500 [1:23:57<10:27, 11.41s/it] 89%|████████▉ | 446/500 [1:24:08<10:13, 11.37s/it] 89%|████████▉ | 447/500 [1:24:20<10:00, 11.34s/it] 90%|████████▉ | 448/500 [1:24:31<09:48, 11.32s/it] 90%|████████▉ | 449/500 [1:24:42<09:36, 11.31s/it] 90%|█████████ | 450/500 [1:24:54<09:26, 11.34s/it]                                                   {'loss': 0.0007, 'grad_norm': 0.00387539342045784, 'learning_rate': 3.1359005254054273e-06, 'epoch': 18.0}
 90%|█████████ | 450/500 [1:24:54<09:26, 11.34s/it] 90%|█████████ | 451/500 [1:25:05<09:21, 11.46s/it] 90%|█████████ | 452/500 [1:25:17<09:07, 11.41s/it] 91%|█████████ | 453/500 [1:25:28<08:54, 11.37s/it] 91%|█████████ | 454/500 [1:25:39<08:41, 11.35s/it] 91%|█████████ | 455/500 [1:25:51<08:29, 11.33s/it] 91%|█████████ | 456/500 [1:26:02<08:18, 11.32s/it] 91%|█████████▏| 457/500 [1:26:13<08:06, 11.31s/it] 92%|█████████▏| 458/500 [1:26:24<07:54, 11.30s/it] 92%|█████████▏| 459/500 [1:26:36<07:43, 11.30s/it] 92%|█████████▏| 460/500 [1:26:47<07:31, 11.30s/it]                                                   {'loss': 0.0017, 'grad_norm': 0.0065573882311582565, 'learning_rate': 2.0343012729971243e-06, 'epoch': 18.4}
 92%|█████████▏| 460/500 [1:26:47<07:31, 11.30s/it] 92%|█████████▏| 461/500 [1:26:58<07:20, 11.30s/it] 92%|█████████▏| 462/500 [1:27:10<07:09, 11.30s/it] 93%|█████████▎| 463/500 [1:27:21<06:57, 11.30s/it] 93%|█████████▎| 464/500 [1:27:32<06:46, 11.29s/it] 93%|█████████▎| 465/500 [1:27:43<06:35, 11.30s/it] 93%|█████████▎| 466/500 [1:27:55<06:23, 11.29s/it] 93%|█████████▎| 467/500 [1:28:06<06:12, 11.29s/it] 94%|█████████▎| 468/500 [1:28:17<06:01, 11.30s/it] 94%|█████████▍| 469/500 [1:28:29<05:50, 11.29s/it] 94%|█████████▍| 470/500 [1:28:40<05:38, 11.29s/it]                                                   {'loss': 0.0009, 'grad_norm': 0.15300467610359192, 'learning_rate': 1.166386083291604e-06, 'epoch': 18.8}
 94%|█████████▍| 470/500 [1:28:40<05:38, 11.29s/it] 94%|█████████▍| 471/500 [1:28:51<05:27, 11.30s/it] 94%|█████████▍| 472/500 [1:29:03<05:16, 11.29s/it] 95%|█████████▍| 473/500 [1:29:14<05:04, 11.29s/it] 95%|█████████▍| 474/500 [1:29:25<04:53, 11.29s/it] 95%|█████████▌| 475/500 [1:29:36<04:43, 11.32s/it] 95%|█████████▌| 476/500 [1:29:48<04:34, 11.44s/it] 95%|█████████▌| 477/500 [1:30:00<04:22, 11.41s/it] 96%|█████████▌| 478/500 [1:30:11<04:10, 11.37s/it] 96%|█████████▌| 479/500 [1:30:22<03:58, 11.34s/it] 96%|█████████▌| 480/500 [1:30:33<03:46, 11.33s/it]                                                   {'loss': 0.0006, 'grad_norm': 0.00417146272957325, 'learning_rate': 5.363833518505834e-07, 'epoch': 19.2}
 96%|█████████▌| 480/500 [1:30:33<03:46, 11.33s/it] 96%|█████████▌| 481/500 [1:30:45<03:34, 11.31s/it] 96%|█████████▋| 482/500 [1:30:56<03:23, 11.30s/it] 97%|█████████▋| 483/500 [1:31:07<03:12, 11.30s/it] 97%|█████████▋| 484/500 [1:31:18<03:00, 11.30s/it] 97%|█████████▋| 485/500 [1:31:30<02:49, 11.30s/it] 97%|█████████▋| 486/500 [1:31:41<02:38, 11.29s/it] 97%|█████████▋| 487/500 [1:31:52<02:26, 11.29s/it] 98%|█████████▊| 488/500 [1:32:04<02:15, 11.28s/it] 98%|█████████▊| 489/500 [1:32:15<02:04, 11.28s/it] 98%|█████████▊| 490/500 [1:32:26<01:52, 11.28s/it]                                                   {'loss': 0.0011, 'grad_norm': 0.2235489934682846, 'learning_rate': 1.4736238865398765e-07, 'epoch': 19.6}
 98%|█████████▊| 490/500 [1:32:26<01:52, 11.28s/it] 98%|█████████▊| 491/500 [1:32:37<01:41, 11.28s/it] 98%|█████████▊| 492/500 [1:32:49<01:30, 11.29s/it] 99%|█████████▊| 493/500 [1:33:00<01:19, 11.29s/it] 99%|█████████▉| 494/500 [1:33:11<01:07, 11.29s/it] 99%|█████████▉| 495/500 [1:33:23<00:56, 11.29s/it] 99%|█████████▉| 496/500 [1:33:34<00:45, 11.29s/it] 99%|█████████▉| 497/500 [1:33:45<00:33, 11.28s/it]100%|█████████▉| 498/500 [1:33:57<00:22, 11.30s/it]100%|█████████▉| 499/500 [1:34:08<00:11, 11.29s/it]100%|██████████| 500/500 [1:34:19<00:00, 11.33s/it]                                                   {'loss': 0.0009, 'grad_norm': 0.0038269481156021357, 'learning_rate': 1.2184647302626583e-09, 'epoch': 20.0}
100%|██████████| 500/500 [1:34:19<00:00, 11.33s/it][INFO|trainer.py:4309] 2025-11-13 23:47:53,381 >> Saving model checkpoint to saves/llama3-8b/lora/sft/checkpoint-500
[INFO|configuration_utils.py:763] 2025-11-13 23:47:53,476 >> loading configuration file /root/autodl-tmp/Llama-33-70B-Instruct/config.json
[INFO|configuration_utils.py:839] 2025-11-13 23:47:53,477 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2421] 2025-11-13 23:47:54,042 >> chat template saved in saves/llama3-8b/lora/sft/checkpoint-500/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-11-13 23:47:54,045 >> tokenizer config file saved in saves/llama3-8b/lora/sft/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-11-13 23:47:54,045 >> Special tokens file saved in saves/llama3-8b/lora/sft/checkpoint-500/special_tokens_map.json
[INFO|trainer.py:2810] 2025-11-13 23:47:55,062 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   {'train_runtime': 5662.794, 'train_samples_per_second': 3.532, 'train_steps_per_second': 0.088, 'train_loss': 0.3843945504426956, 'epoch': 20.0}
100%|██████████| 500/500 [1:34:21<00:00, 11.33s/it]100%|██████████| 500/500 [1:34:21<00:00, 11.32s/it]
[INFO|trainer.py:4309] 2025-11-13 23:47:55,065 >> Saving model checkpoint to saves/llama3-8b/lora/sft
[INFO|configuration_utils.py:763] 2025-11-13 23:47:55,174 >> loading configuration file /root/autodl-tmp/Llama-33-70B-Instruct/config.json
[INFO|configuration_utils.py:839] 2025-11-13 23:47:55,175 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2421] 2025-11-13 23:47:55,612 >> chat template saved in saves/llama3-8b/lora/sft/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-11-13 23:47:55,615 >> tokenizer config file saved in saves/llama3-8b/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-11-13 23:47:55,615 >> Special tokens file saved in saves/llama3-8b/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =        20.0
  total_flos               = 775487060GF
  train_loss               =      0.3844
  train_runtime            =  1:34:22.79
  train_samples_per_second =       3.532
  train_steps_per_second   =       0.088
Figure saved at: saves/llama3-8b/lora/sft/training_loss.png
[WARNING|2025-11-13 23:47:55] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.
[WARNING|2025-11-13 23:47:55] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|modelcard.py:456] 2025-11-13 23:47:55,939 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[W1113 23:47:56.937384817 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1113 23:47:56.943434655 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1113 23:47:56.953675239 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1113 23:47:56.095423449 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1113 23:47:57.974902728 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1113 23:47:58.877091075 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1113 23:47:58.519387515 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
